EC 4.4: How to improve end-to-end latency of chains?

Currently, the EDF scheduling approach we use is chain-agnostic, in that it only cares about individual deadlines. If we wanted to improve the end-to-end latency of chains, we could make it chain-aware by taking into account `chain_id` and `chain_pos` whe nassigning priorities. We could:
    1. favor earlier stages so that data flows through the pipeline faster
    2. partition end-to-end deadlines across chains rather than giving each task independent deadlines
Making these changes would reduce e2e latency because, suppose we have task chain T with tasks t1->t2->t3. If t2 cannot make progress until t1 produces its output, if the scheduler runs t2 first (which is possible given the arbitrary deadlines), t2 would immediately block or spin waiting for data from t1, meaning CPU resources are guaranteed to be wasted. With these changes, we know t1 will always run before t2, so t2 will never have the chance to block the CPU and waste resources.

Writeup 4.5:

Q1. What is the difference between partitioned and global multi-processor scheduling? What are the
pros and cons of each?

Partitioned multi-processor scheduling statically binds each task to a single processor. Because each task is predictably bound, it is an ideal candidate for hard real-time systems that need guaranteed deadlines and simple analysis. One major drawback is that it can easily lead to processor imbalance if the tasks aren't distributed in a near-optimal way.

Global multi-processor scheduling holds a global queue from which any processor can run a task. This improves the overall load balancing issues from partitioned scheduling, but introduces higher runtime overhead from context switching and migration.


Q2. Explain your task partitioning strategy. List which tasks you assigned to Core 0 and Core 1, and
show that the utilization on each core is <= 1.0

We used First Fit Decreasing (FFD) for our task partitioning strategy. This strategy works by first sorting all tasks by CPU utilization. It then starts from the highest utilization and works it way down to the lowest utilization tasks, assigning them to the first core that they fit into.

We placed tasks 2, 7, and 8 into core 0 and tasks 0, 1, 3, 4, 5, and 6 into core 1. Please see below for the utilization of each task/core and how FFD was able to successfully keep the utilization on each core <= 1.0.

Original Order:
Task 0: 17 100 100 0 0 | UTIL: 0.17
Task 1: 12 200 200 0 1 | UTIL: 0.06
Task 2: 23 50 50 0 2   | UTIL: 0.46
Task 3: 25 150 150 1 0 | UTIL: 0.1533333
Task 4: 17 100 100 1 1 | UTIL: 0.17
Task 5: 42 500 500 1 2 | UTIL: 0.084
Task 6: 30 300 300 2 0 | UTIL: 0.10
Task 7: 28 100 100 2 1 | UTIL: 0.28
Task 8: 42 200 200 2 2 | UTIL: 0.21

Sorted Order Separated by Core:
Core 0 | TOTAL UTIL: 0.95
Task 2: 23 50 50 0 2   | UTIL: 0.46
Task 7: 28 100 100 2 1 | UTIL: 0.28
Task 8: 42 200 200 2 2 | UTIL: 0.21

Core 1 | TOTAL UTIL: 0.73733333
Task 0: 17 100 100 0 0 | UTIL: 0.17
Task 4: 17 100 100 1 1 | UTIL: 0.17
Task 3: 25 150 150 1 0 | UTIL: 0.1533333
Task 6: 30 300 300 2 0 | UTIL: 0.10
Task 5: 42 500 500 1 2 | UTIL: 0.084
Task 1: 12 200 200 0 1 | UTIL: 0.06

Q3. Please measure the overhead of your EDF scheduling implementation in the Kernel space? Was it
acceptable?



Q4. How did you implement the e2e latency measurement? Specifically, where in the kernel are the
start and end times captured, and how did you handle synchronization between cores?
The start time of the first task of the chain is captured in the kernel/sched/core.c, when a new task is entered, if the chain has just been restarted and we are just started the first task of the chain, the current time is recorded. The end times are recorded in proj4/kernel/reservation.c; specifically, in the wait_until_next_period syscall right after we check if there is an active reservation. This was chosen because we know that this should only be called right after a task is completed but before we've waited until the period to end. The synchronization between cores and tasks within the chain is handled with a new "chain_struct" which holds essential chain information like the number of tasks, the next task which is able to be run after completion of the current task, max latency, min latency, etc. The task_struct has a chain_struct pointer which points to a dedicated struct for each unique chain_id. This chain_struct is shared with all of the tasks belonging to the same chain so that they can read and write information regarding their respective chains from different cores and processes.


Q5. What issues did you face for this project and how did you resolve them?

One issue that we ran into was the fact that we only had one pi and so we couldn't all test things on demand. We weren't able to completely resolve this issue, but we mitigated it by communicating whenever we made major changes and requested that Cameron (who physically had possession of the pi) test the code.

Another issue that we ran into was our lack of experience with errors specific to this type of project. Although we have started to understand better the general sort of way that different parts of the code come together to make everything function properly, there are still many unique types of errors we haven't experienced before which have led us to have to do research to understand what specifically is going wrong. A couple of examples of errors that we ran into were stack smashing and kernel panic, both of which we had never run into before because we've worked almost exclusively within our IDEs in the past. We resolved this issue by looking into forums like StackOverflow to find other people who have had similar issues and by reading through documentation on websites like https://docs.kernel.org/.
